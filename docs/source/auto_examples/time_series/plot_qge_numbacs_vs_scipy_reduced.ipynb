{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# NumbaCS vs SciPy/NumPy -- QGE\n\nCompare run times for flow map and FTLE between NumbaCS and\na pure SciPy/NumPy implementation for the QGE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: ajarvis\n# Hardware: Intel(R) Core(TM) i7-3770K CPU @ 3.50GHz, Cores = 4, Threads = 8\n\nimport numpy as np\nfrom scipy.interpolate import RegularGridInterpolator\nfrom scipy.integrate import odeint\nfrom numbacs.integration import flowmap_grid_2D\nfrom numbacs.flows import get_interp_arrays_2D, get_flow_2D\nfrom numbacs.diagnostics import ftle_grid_2D\nimport time\nfrom math import copysign, log\nfrom multiprocessing import Pool\nfrom functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get flow data and create NumbaCS flow\nLoad velocity data, set up domain, set the integration span and direction, create\ninterpolant of velocity data and retrieve necessary arrays.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load in qge velocity data\nu = np.load(\"../data/qge/qge_u.npy\")\nv = np.load(\"../data/qge/qge_v.npy\")\n\n# set up domain\nnt, nx, ny = u.shape\nx = np.linspace(0, 1, nx)\ny = np.linspace(0, 2, ny)\nt = np.linspace(0, 1, nt)\ndx = x[1] - x[0]\ndy = y[1] - y[0]\n\n# use reduced domain or scipy will take much too long\ns = 4\nx = x[::s]\ny = y[::s]\nt = t[::s]\nu = u[::s, ::s, ::s]\nv = v[::s, ::s, ::s]\n\nnx = len(x)\nny = len(y)\n\n# set integration span and integration direction\nt0 = 0.0\nT = 0.1\nparams = np.array([copysign(1, T)])  # important this is an array of type float\n\n# get interpolant arrays of velocity field\ngrid_vel, C_eval_u, C_eval_v = get_interp_arrays_2D(t, x, y, u, v)\n\n# get flow to be integrated\nfuncptr = get_flow_2D(grid_vel, C_eval_u, C_eval_v, extrap_mode=\"linear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scipy interpolant and ODE function\nCreate interpolant and function for SciPy ode solver.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ui = RegularGridInterpolator((t, x, y), u, method=\"linear\", bounds_error=False, fill_value=0.0)\nvi = RegularGridInterpolator((t, x, y), v, method=\"linear\", bounds_error=False, fill_value=0.0)\n\n\ndef odeint_fun(yy, tt):\n    pt = np.array([tt, yy[0], yy[1]])\n\n    return ui(pt)[0], vi(pt)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SciPy flow map and FTLE functions\nCreate functions to compute flow maps and FTLE using standard SciPy/Numpy methods.\nUses scipy.integrate.odeint (implements LSODA method) for particle integration.\nThe scipy.integrate.solve_ivp function is newer and allows the use of other solvers\nbut odeint is faster even when solve_ivp uses LSODA as its method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tspan = np.array([t0, t0 + T])\n\n\ndef scipy_odeint_flowmap_par(t0, y0):\n    tspan = np.array([t0, t0 + T])\n    sol = odeint(odeint_fun, y0, tspan, rtol=1e-6, atol=1e-8)\n    flowmap = sol[-1, :]\n\n    return flowmap\n\n\ndef numpy_ftle_par(fm, inds):\n    i, j = inds\n    absT = abs(T)\n    dxdx = (fm[i + 1, j, 0] - fm[i - 1, j, 0]) / (2 * dx)\n    dxdy = (fm[i, j + 1, 0] - fm[i, j - 1, 0]) / (2 * dy)\n    dydx = (fm[i + 1, j, 1] - fm[i - 1, j, 1]) / (2 * dx)\n    dydy = (fm[i, j + 1, 1] - fm[i, j - 1, 1]) / (2 * dy)\n\n    off_diagonal = dxdx * dxdy + dydx * dydy\n    C = np.array([[dxdx**2 + dydx**2, off_diagonal], [off_diagonal, dxdy**2 + dydy**2]])\n\n    max_eig = np.linalg.eigvalsh(C)[-1]\n    if max_eig > 1:\n        ftle = 1 / (2 * absT) * log(max_eig)\n    else:\n        ftle = 0\n\n    return ftle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute SciPy/Numpy flow map, FTLE\nCompute flowmap, FTLE, and calculate run times for the SciPy/NumPy implementation.\nFor this problem on this hardware, computing flow map and FTLE parallel in space\n(as opposed to parallel in time) was the faster implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set initial conditions\nn = 2\nt0span = np.linspace(0, 0.1, n)\n[X, Y] = np.meshgrid(x, y, indexing=\"ij\")\nY0 = np.column_stack((X.ravel(), Y.ravel()))\nsftle = np.zeros((n, nx - 2, ny - 2), np.float64)\n\n# set parallel pool to use maximum number of threads for this hardware,\n# open pool\nnum_threads = 8\npl = Pool(num_threads)\n\n# create inds to pass to ftle function\nxinds = np.arange(1, nx - 1)\nyinds = np.arange(1, ny - 1)\n[I, J] = np.meshgrid(xinds, yinds, indexing=\"ij\")\ninds = np.column_stack((I.ravel(), J.ravel()))\n\n# compute flowmap and ftle parallel in space\nsfmtt = 0\nsftt = 0\n\nfor k, t0 in enumerate(t0span):\n    ks = time.perf_counter()\n    func = partial(scipy_odeint_flowmap_par, t0)\n    res = np.array(pl.map(func, Y0)).reshape(nx, ny, 2)\n    kf = time.perf_counter()\n    sfmtt += kf - ks\n\n    fks = time.perf_counter()\n    func2 = partial(numpy_ftle_par, res)\n    sftle[k, :, :] = np.array(pl.map(func2, inds)).reshape(nx - 2, ny - 2)\n    fkf = time.perf_counter()\n    sftt += fkf - fks\n\npl.close()\npl.terminate()\n\nprint(\"SciPy/NumPy flowmap and FTLE took \" + f\"{sfmtt + sftt:.5f} seconds for {n} iterates\")\nprint(\"Mean time for SciPy/NumPy flowmap and FTLE -- \" + f\"{(sfmtt + sftt) / n:.5f} seconds\\n\")\nprint(f\"Scipy flowmap took {sfmtt:.5} seconds for {n:1d} iterates\")\nprint(f\"Mean time for Scipy flowmap -- {sfmtt / n:.5} seconds\\n\")\nprint(f\"NumPy ftle took {sftt:.5} seconds for {n:1d} iterates\")\nprint(f\"Mean time for NumPy ftle -- {sftt / n:.5} seconds\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute NumbaCS flow map, FTLE\nCompute flowmap, FTLE, and calculate run times for the NumbaCS implementation.\nFor this problem on this hardware, computing flow map and FTLE parallel in space\n(as opposed to parallel in time) was the faster implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ftle = np.zeros((n, nx, ny), np.float64)\n\n# first call and record warmup times\nwfm = time.perf_counter()\nflowmap_wu = flowmap_grid_2D(funcptr, t0, T, x, y, params)\nwu_fm = time.perf_counter() - wfm\nprint(f\"Flowmap with warm-up took {wu_fm:.5f} seconds\")\n\nwf = time.perf_counter()\nftle[0, :, :] = ftle_grid_2D(flowmap_wu, T, dx, dy)\nwu_f = time.perf_counter() - wf\nprint(f\"FTLE with warm-up took {wu_f:.5f} seconds\")\n\n# initialize runtime counters\nfmtt = wu_fm\nftt = wu_f\n\n# loop over initial times, compute flowmap and ftle\nfor k, t0 in enumerate(t0span[1:]):\n    ks = time.perf_counter()\n    flowmap = flowmap_grid_2D(funcptr, t0, T, x, y, params)\n    kf = time.perf_counter()\n    fmtt += kf - ks\n\n    fks = time.perf_counter()\n    ftle[k, :, :] = ftle_grid_2D(flowmap, T, dx, dy)\n    fkf = time.perf_counter()\n    ftt += fkf - fks\n\nprint(\"NumbaCS flowmap and FTLE took \" + f\"{fmtt + ftt:.5f} for {n:1d} iterates\")\nprint(f\"Mean time for flowmap and FTLE -- {(fmtt + ftt) / n:.5f} seconds (w/ warmup)\")\nprint(\n    \"Mean time for flowmap and FTLE -- \"\n    + f\"{(fmtt - wu_fm + ftt - wu_f) / (n - 1):.5f} seconds (w/o warmup)\\n\"\n)\nprint(f\"NumbaCS flowmap_grid_2D took {fmtt:.5f} seconds for {n:1d} iterates\")\nprint(f\"Mean time for flowmap_grid_2D -- {fmtt / n:.5f} seconds (w/ warmup)\")\nprint(\n    \"Mean time for flowmap_grid_2D -- \" + f\"{(fmtt - wu_fm) / (n - 1):.5f} seconds (w/o warmup)\\n\"\n)\nprint(f\"NumbaCS ftle_grid_2D took {ftt:.5f} seconds for {n:1d} iterates\")\nprint(f\"Mean time for ftle_grid_2D -- {ftt / n:.5f} seconds (w/ warmup)\")\nprint(f\"Mean time for ftle_grid_2D -- {(ftt - wu_f) / (n - 1):.5f} seconds (w/o warmup)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare timings\nCompare timings and quantify speed-up. The second and third columns quantify the\nspeed-up gained using NumbaCS. The second column includes warm-up time, the speed-up\nwould increase as *n* grows larger. The third column ignores the warm-up time\nand quantifies the speed-up as *n* goes to infinity and the warm-up time becomes\nnegligible. This represents the theoretical speed-up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stt = sfmtt + sftt\nntt = fmtt + ftt\n\nstpi = (sfmtt + sftt) / n\nntpi = (ntt - wu_fm - wu_f) / (n - 1)\n\nd1 = 5\nd2 = 2\ndata = [\n    [round(stt, d1), \"--\", \"--\"],\n    [round(ntt, d1), round(stt / ntt, d2), round(stpi / ntpi, d2)],\n]\n\ntimes = [f\"total time (n={n})\", \"speedup\", \"speedup (n->inf)\"]\nmethods = [\"SciPy/NumPy\", \"NumbaCS\"]\n\nformat_row = \"{:>25}\" * (len(data[0]) + 1)\n\nprint(format_row.format(\"\", *times))\n\nfor name, vals in zip(methods, data):\n    print(format_row.format(name, *vals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The SciPy interpolation package creates a bottleneck when used to solve odes and has\n   a large effect on the overall runtime. For this reason, we only run for 2\n   iterates or the code would take much too long. As *n* increaes, the speed-up\n   would increase quite quickly as the warm-up time of the NumbaCS implementation\n   becomes less significant. Regardless, the NumbaCS implementation achieves a\n   drastic speed-up when used on numerical velocity data. This is largely achieved\n   by the numbalsoda and interpolation packages, both of which utilize Numba.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}